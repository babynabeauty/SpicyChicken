{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms, datasets, models\n",
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import sys\n",
    "import platform\n",
    "from torchvision.models import mobilenet_v2\n",
    "import model_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(os.getcwd(), \"dataset_class4\")\n",
    "json_path = 'garbage_classify_rule.json'\n",
    "assert os.path.exists(json_path), \"file: '{}' dose not exist.\".format(json_path)\n",
    "with open(json_path, 'r', encoding = \"utf8\") as f:\n",
    "    class_indict = json.load(f)\n",
    "nums = ['其他垃圾', '厨余垃圾', '可回收物', '有害垃圾']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将华为云数据按照标签进行分类整理\n",
    "if not os.path.exists(dataset_path):\n",
    "    os.mkdir(dataset_path)\n",
    "assert os.path.exists(dataset_path), \"dataset path not exists\"\n",
    "for root, dirs, files in os.walk(os.path.join(os.getcwd(), \"train_data\")):\n",
    "    for file in tqdm(files):\n",
    "        if(os.path.splitext(file)[1]=='.txt'):\n",
    "            with open(os.path.join(root, file), \"r\") as f:\n",
    "                file_name, classify = f.readlines()[0].split(\",\")\n",
    "                file_name = file_name.strip()\n",
    "                classify = classify.strip()\n",
    "                classify = str(nums.index(class_indict[classify].split(\"/\")[0]))\n",
    "                if not os.path.exists(os.path.join(dataset_path, classify)):\n",
    "                    os.mkdir(os.path.join(dataset_path, classify))\n",
    "                shutil.copyfile(os.path.join(root, file_name), os.path.join(dataset_path, classify, file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计类别\n",
    "classify_num = len(nums)\n",
    "print(\"there exist {} classes in this datasets\".format(classify_num))\n",
    "for root, dirs, files in os.walk(dataset_path):\n",
    "    if(len(files) == 0):\n",
    "        continue\n",
    "    # print(class_indict[os.path.split(root)[-1]], len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集处理\n",
    "data_transform = {\n",
    "    \"train\": transforms.Compose([transforms.Resize(224),\n",
    "                                transforms.CenterCrop(224),\n",
    "                                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                transforms.RandomVerticalFlip(p=0.5),\n",
    "                                transforms.RandomRotation(45),\n",
    "                                transforms.RandomGrayscale(p=0.5),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
    "    \"val\": transforms.Compose([transforms.Resize(256),\n",
    "                                transforms.CenterCrop(224),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练集测试集划分\n",
    "\n",
    "# 选择设备\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 载入图片并进行训练集和测试集划分\n",
    "split_rate = 0.8\n",
    "dataset = datasets.ImageFolder(root = dataset_path, transform = data_transform['train']) #对图像进行处理\n",
    "train_size = int(len(dataset) * split_rate)\n",
    "valid_size = len(dataset) - train_size\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(dataset, (train_size, valid_size))\n",
    "print(\"using {} images for training, {} images for validation.\".format(train_size, valid_size))\n",
    "\n",
    "# 构建data_loader\n",
    "batch_size = 16\n",
    "num_work = min([os.cpu_count(), batch_size if batch_size > 1 else 0, 8])  # number of workers\n",
    "# windows num_work = 0\n",
    "num_work = 0 if platform.system().lower() == \"windows\" else num_work\n",
    "print(\"using {} num_work for training and validation.\".format(num_work))\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                            batch_size=batch_size, shuffle=True,\n",
    "                                            num_workers=num_work)\n",
    "\n",
    "validate_loader = torch.utils.data.DataLoader(valid_dataset,\n",
    "                                            batch_size=batch_size, shuffle=False,\n",
    "                                            num_workers=num_work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 网络构建\n",
    "nets = [\"resnet18\", \"resnet50\", \"resnet101\", \"mobilenet_v2\", \"mobilenet_v3\"]\n",
    "choose_net = nets[3]\n",
    "print(\"using {} net to train!\".format(choose_net))\n",
    "if(choose_net == \"resnet18\"):\n",
    "    net = models.resnet18(pretrained = False) # 这里可以使用True直接下载 \n",
    "elif choose_net == \"resnet50\":\n",
    "    net = models.resnet50(pretrained = False)\n",
    "elif choose_net == \"resnet101\":\n",
    "    net = models.resnet101(pretrained = False)\n",
    "elif choose_net == \"mobilenet_v2\":\n",
    "    net = models.mobilenet_v2(pretrained = False)\n",
    "elif choose_net == \"mobilenet_v3\":\n",
    "    net = model_v3.mobilenet_v3_large(num_classes = classify_num)\n",
    "\n",
    "# 载入权重\n",
    "model_weight_path = \"{name}-pre.pth\".format(name = choose_net)\n",
    "assert os.path.exists(model_weight_path), \"file {} does not exist.\".format(model_weight_path)\n",
    "net.load_state_dict(torch.load(model_weight_path, map_location=device))\n",
    "\n",
    "# 对于模型的每个权重，使其不进行反向传播，即固定参数\n",
    "for param in net.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 修改最后一层，更换输出类别数\n",
    "if(\"resnet\" in choose_net):\n",
    "    # 不固定最后一层，即全连接层fc\n",
    "    for param in net.fc.parameters():\n",
    "        param.requires_grad = True\n",
    "    channel_in = net.fc.in_features #获取fc层的输入通道数\n",
    "    net.fc = nn.Linear(channel_in, classify_num)\n",
    "elif(\"mobilenet_v2\" in choose_net):\n",
    "    # 不固定最后一层，即classify\n",
    "    for param in net.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "    channel_in = net.last_channel #获取classify层的输入通道数\n",
    "    net.classifier = nn.Sequential(nn.Dropout(0.2), nn.Linear(channel_in, classify_num))\n",
    "else:\n",
    "    pass\n",
    "\n",
    "net.to(device)\n",
    "\n",
    "# 设定损失函数\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# 优化器过滤\n",
    "optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, net.parameters()), lr=0.001, momentum=0.9)\n",
    "# params = [p for p in net.parameters() if p.requires_grad]\n",
    "# optimizer = torch.optim.Adam(params, lr=0.0003)\n",
    "save_path = 'garbage_{}_class4.pkl'.format(choose_net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练\n",
    "losslist=[]\n",
    "epochs = 20\n",
    "best_acc = 0.0\n",
    "for epoch in range(epochs):\n",
    "    # train\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    # use tqdm\n",
    "    train_bar = tqdm(train_loader, file=sys.stdout)\n",
    "    for step, data in enumerate(train_bar):\n",
    "        images, labels = data\n",
    "        # make commulate grad to zero\n",
    "        optimizer.zero_grad()\n",
    "        output = net(images.to(device))\n",
    "        loss = loss_function(output, labels.to(device))\n",
    "        \n",
    "        # loss backward\n",
    "        loss.backward()\n",
    "        \n",
    "        # update parameter grad\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        train_bar.desc = \"train epoch[{}/{}] loss:{:.3f}\".format(epoch + 1, epochs, loss)\n",
    "        \n",
    "        # print statistics for every 200 batch_size\n",
    "        # if step % 200 == 199:\n",
    "        #     print('[%d %5d] loss: %.3f' % (epoch + 1, step + 1, running_loss / 200))\n",
    "        #     losslist.append(running_loss / 200)\n",
    "        #     running_loss = 0.0\n",
    "    \n",
    "    # eval\n",
    "    net.eval()\n",
    "    acc = 0.0\n",
    "    with torch.no_grad():\n",
    "        valid_bar = tqdm(validate_loader, file=sys.stdout)\n",
    "        for val_data in valid_bar:\n",
    "            val_images, val_labels = val_data\n",
    "            outputs = net(val_images.to(device))\n",
    "            loss = loss_function(outputs, val_labels.to(device))\n",
    "            predict = torch.max(outputs, dim=1)[1]\n",
    "            acc += torch.eq(predict, val_labels.to(device)).sum().item()\n",
    "            valid_bar.desc = \"valid epoch[{}/{}] loss:{:.3f}\".format(epoch + 1, epochs, loss)\n",
    "    val_accurate = acc / valid_size\n",
    "    print('[epoch %d] train_loss: %.3f  val_accuracy: %.3f\\n' % (epoch + 1, running_loss / train_size, val_accurate))\n",
    "\n",
    "    if val_accurate > best_acc:\n",
    "        best_acc = val_accurate\n",
    "        torch.save(net, save_path)\n",
    "\n",
    "print(\"Finished Training\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test and plot\n",
    "model_path = \"garbage_resnet18_class4.pkl\"\n",
    "# load image\n",
    "img_path = \"test_pic/3/yaqian.jpg\"\n",
    "assert os.path.exists(img_path), \"file: '{}' dose not exist.\".format(img_path)\n",
    "img = Image.open(img_path)\n",
    "plt.imshow(img)\n",
    "\n",
    "# [N, C, H, W]\n",
    "img = data_transform['val'](img)\n",
    "img = torch.unsqueeze(img, dim=0)\n",
    "\n",
    "# read class_indict\n",
    "assert os.path.exists(json_path), \"file: '{}' dose not exist.\".format(json_path)\n",
    "with open(json_path, 'r', encoding = \"utf8\") as f:\n",
    "    class_indict = json.load(f)\n",
    "\n",
    "# create model\n",
    "assert os.path.exists(model_path), \"file: '{}' dose not exist.\".format(model_path)\n",
    "model = torch.load(model_path)\n",
    "\n",
    "cla_dict = dict((val, key) for key, val in dataset.class_to_idx.items())\n",
    "\n",
    "# prediction\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # predict class\n",
    "    output = torch.squeeze(model(img.to(device))).cpu()\n",
    "    predict = torch.softmax(output, dim=0)\n",
    "    predict_cla = torch.argmax(predict).numpy()\n",
    "\n",
    "# print_res = \"class: {}   prob: {:.3}\".format(class_indict[str(predict_cla)], predict[predict_cla].numpy())\n",
    "print(nums[int(cla_dict[predict_cla.item()])])\n",
    "# for i in range(len(predict)):\n",
    "#     print(\"class: {:10}   prob: {:.3}\".format(class_indict[str(i)], predict[i].numpy()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试集\n",
    "test_dataset = datasets.ImageFolder(root = 'test_pic',transform = data_transform['val'])\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size =5,shuffle =  False,num_workers = 0)\n",
    "net = torch.load('garbage_resnet50_class4.pkl')\n",
    "\n",
    "# 测试集可能不会包含所有类别，需要重新进行class和idx映射\n",
    "test_indict = dict((val, key) for key, val in test_dataset.class_to_idx.items())\n",
    "\n",
    "# unnormalize\n",
    "mean_tensor = torch.from_numpy(np.array([0.485, 0.456, 0.406])).reshape(3, 1, 1).to(torch.float32)\n",
    "var_tensor = torch.from_numpy(np.array([0.229, 0.224, 0.225])).reshape(3, 1, 1).to(torch.float32)\n",
    "\n",
    "for data in test_loader:  \n",
    "    images, labels = data\n",
    "    images, labels= images.to(device),labels.to(device)\n",
    "    outputs = net(images)\n",
    "    # return data and index\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    # total += labels.size(0)\n",
    "    # correct += (predicted == labels).sum()\n",
    "    labels = labels.cpu().numpy()\n",
    "    predicted = predicted.cpu().numpy()\n",
    "    for i in range(len(labels)):\n",
    "        print(\"lable:{}\\npredict:{}\\n\".format(class_indict[test_indict[labels[i]]] , nums[int(cla_dict[predicted[i]])]))\n",
    "        image = images[i].cpu() * var_tensor + mean_tensor\n",
    "        img = transforms.ToPILImage()(image)\n",
    "        display(img)  \n",
    "    # for i in range(len(labels)):\n",
    "    #     if(nums[labels[i]] != nums[predicted[i]]):\n",
    "            \n",
    "# print('Accuracy of the network on the test images: %d %%' % (100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
